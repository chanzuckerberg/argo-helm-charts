# Grafana Alloy Helm Chart Values
# This chart deploys Grafana Alloy with customizable configuration

enabled: true # @schema description: Enable or disable the Grafana Alloy deployment

clusterName: "" # @schema description: Name of the cluster where this chart is deployed. This value is required.; required: true

# Loki endpoints configuration
loki:
  endpoints: # @schema description: List of Loki endpoints to send logs to
    - url: "http://loki.loki.svc:3100/loki/api/v1/push" # @schema description: URL of the Loki endpoint
      # basicAuth: # @schema description: Basic authentication configuration (optional)
      #   envVar: "BASIC_AUTH_CREDENTIALS" # @schema description: Name of the environment variable containing basic auth credentials

# Example with multiple endpoints:
# loki:
#   endpoints:
#     - url: "http://loki.loki.svc:3100/loki/api/v1/push"
#     - url: "https://loki.prod-central-o11y.prod.czi.team/loki/api/v1/push"
#       basicAuth:
#         envVar: "BASIC_AUTH_CREDENTIALS"

# Prometheus remote write configuration
prometheusRemoteWrite:
  enabled: false # @schema description: Enable Prometheus remote write
  endpoints: [] # @schema description: Array of remote write endpoints
    # - url: "" # @schema description: URL of the Prometheus remote write endpoint
    #   basicAuth: # @schema description: Basic authentication configuration (optional)
    #     usernameEnvVar: "MIMIR_USERNAME" # @schema description: Name of the environment variable containing username
    #     passwordEnvVar: "MIMIR_PASSWORD" # @schema description: Name of the environment variable containing password
    #   externalLabels: {} # @schema description: External labels to add to all metrics for this endpoint
    #   queueConfig: # @schema description: Queue configuration for remote write
    #     maxSamplesPerSend: 3000 # @schema description: Maximum samples per send
    #     batchSendDeadline: "10s" # @schema description: Batch send deadline
    #     minShards: 4 # @schema description: Minimum shards
    #     maxShards: 200 # @schema description: Maximum shards
    #     capacity: 10000 # @schema description: Queue capacity
    #   sigv4: # @schema description: SigV4 configuration for AWS authentication
    #     enabled: false # @schema description: Enable SigV4 authentication
    #     region: "" # @schema description: AWS region for SigV4
  metricsFilter: # @schema description: Metrics filtering configuration
    enabled: false # @schema description: Enable metrics filtering
    regex: "" # @schema description: Regex pattern to match metric names to keep

  # Scrape kube-state-metrics for kube_* metrics (e.g. kube_pod_status_phase).
  scrapeKubeStateMetrics: # @schema description: Scrape kube-state-metrics service for kube_* metrics
    enabled: true # @schema description: Enable scraping kube-state-metrics (requires alloyConfig.metrics.enabled and prometheusRemoteWrite.enabled)
    namespace: "kube-system" # @schema description: Namespace where kube-state-metrics service runs
    serviceSelector: "app.kubernetes.io/name=kube-state-metrics" # @schema description: Label selector (key=value) to find the kube-state-metrics service

  # Scrape kubelet for kubelet_* and kubernetes_build_info metrics.
  scrapeKubelet: # @schema description: Scrape kubelet metrics from each node (kubelet_*, kubernetes_build_info)
    enabled: true # @schema description: Enable scraping kubelet metrics (requires alloyConfig.metrics.enabled and prometheusRemoteWrite.enabled)

  # Scrape cadvisor for container_* metrics (CPU, memory, network, filesystem).
  scrapeCadvisor: # @schema description: Scrape cadvisor metrics from each node (container_*)
    enabled: true # @schema description: Enable scraping cadvisor metrics (requires alloyConfig.metrics.enabled and prometheusRemoteWrite.enabled)

  # Scrape Kubernetes service endpoints (e.g., hubble-metrics, cilium-agent) for metrics.
  # NOTE: If you're migrating from loki-stack's Prometheus server, you should disable endpoint scraping there.
  # In loki-stack values, set: prometheus.server.extraScrapeConfigs: [] or remove endpoint scraping configs.
  scrapeEndpoints: # @schema description: Scrape Kubernetes service endpoints with prometheus.io/scrape annotations
    enabled: true # @schema description: Enable scraping service endpoints (requires alloyConfig.metrics.enabled and prometheusRemoteWrite.enabled)
    scrapeInterval: "1m" # @schema description: Scrape interval for endpoint scraping
    scrapeTimeout: "10s" # @schema description: Scrape timeout for endpoint scraping

  # Static blackbox exporter targets (for targets not discovered via Kubernetes service annotations)
  scrapeBlackboxStatic: # @schema description: Scrape static targets using blackbox exporter
    enabled: false # @schema description: Enable static blackbox scraping (requires alloyConfig.metrics.enabled and prometheusRemoteWrite.enabled)
    scrapeInterval: "1m" # @schema description: Scrape interval for static blackbox scraping
    scrapeTimeout: "10s" # @schema description: Scrape timeout for static blackbox scraping
    config: | # @schema description: Blackbox exporter configuration in YAML format (defaults to standard modules if not provided)
      modules:
        http_2xx:
          prober: http
          timeout: 5s
          http:
            valid_status_codes: []
            method: GET
        https_2xx:
          prober: http
          timeout: 5s
          http:
            valid_status_codes: []
            method: GET
        tcp_connect:
          prober: tcp
          timeout: 5s
        icmp:
          prober: icmp
          timeout: 5s
    targets: [] # @schema description: List of static blackbox targets to probe
      # Example:
      # - job_name: "blackbox-internal"
      #   targets:
      #     - "http://loki.loki.svc.cluster.local:3100/ready"
      #   module: "http_2xx"
      #   blackboxExporterAddress: "blackbox-exporter.loki.svc.cluster.local:9115"
      #   probe_type: "internal"

# Custom Alloy configuration to merge/override the default configmap
# This will be used to create a custom grafana-alloy-config ConfigMap
alloyConfig:
  # @schema description: Custom Alloy configuration content (River format). If empty, uses default collection config.
  # See https://grafana.com/docs/alloy/latest/reference/config-blocks/
  content: ""

  # @schema description: Logging configuration for Alloy
  logging:
    level: info # @schema enum: [debug, info, warn, error]; description: Log level for Alloy
    format: logfmt # @schema enum: [logfmt, json]; description: Log format for Alloy
  
  # @schema description: Enable pod log collection
  podLogs:
    enabled: true # @schema description: Enable collection of pod logs from all nodes
  
  # @schema description: Enable Kubernetes events collection
  events:
    enabled: true # @schema description: Enable collection of Kubernetes events
  
  # @schema description: Enable Prometheus metrics collection
  metrics:
    enabled: false # @schema description: Enable scraping of Prometheus metrics from pods (requires prometheusRemoteWrite.enabled=true)

  # @schema description: Enable Beyla integration for eBPF-based application instrumentation
  beyla:
    enabled: false # @schema description: Enable Beyla integration (requires prometheusRemoteWrite.enabled=true)
    debug: false # @schema description: Enable debug logging for Beyla BPF component

# Alloy subchart configuration
# Reference: https://github.com/grafana/alloy/tree/main/operations/helm/charts/alloy
alloy:
  # @schema description: Alloy deployment configuration
  alloy:
    # @schema description: Alloy container image configuration
    image:
      registry: docker.io # @schema description: Container registry for Alloy image
      repository: grafana/alloy # @schema description: Container image repository
      # tag: defaults to appVersion from alloy subchart # @schema description: Container image tag (Alloy version)
      pullPolicy: IfNotPresent # @schema enum: [Always, IfNotPresent, Never]; description: Image pull policy

    # @schema description: ConfigMap configuration for Alloy
    configMap:
      create: false # @schema description: Whether to create the default ConfigMap (false since we create our own)
      name: grafana-alloy-config # @schema description: Name of the ConfigMap to use
      key: config.alloy # @schema description: Key in the ConfigMap containing the configuration

    # @schema description: Extra arguments to pass to Alloy
    extraArgs: []

    # @schema description: Extra environment variables
    extraEnv: []

    # @schema description: Extra ports to expose
    extraPorts: []

    # @schema description: Clustering configuration
    clustering:
      enabled: false # @schema description: Enable clustering mode

    # @schema description: Stabilize outgoing connection when clustering is enabled
    stabilityLevel: generally-available # @schema description: Stability level for Alloy features; enum: [experimental, public-preview, generally-available]

  # @schema description: Controller configuration
  controller:
    type: deployment # @schema enum: [daemonset, deployment, statefulset]; description: Type of controller to use
    replicas: 1 # @schema description: Number of replicas (only for deployment/statefulset)

  # @schema description: Service configuration
  service:
    enabled: true # @schema description: Enable the service
    type: ClusterIP # @schema description: Service type

  # @schema description: Service account configuration
  serviceAccount:
    create: true # @schema description: Create a service account
    name: "grafana-alloy" # @schema description: Name of the service account
    annotations: {} # @schema description: Annotations to add to the service account

  # @schema description: RBAC configuration
  rbac:
    create: true # @schema description: Create RBAC resources

  # @schema description: Pod security context
  podSecurityContext: {}

  # @schema description: Config reloader configuration
  configReloader:
    enabled: true # @schema description: Enable config reloader

  # @schema description: Container security context
  securityContext:
    runAsNonRoot: true # @schema description: Run container as non-root user
    allowPrivilegeEscalation: false # @schema description: Prevent privilege escalation
    readOnlyRootFilesystem: true # @schema description: Mount root filesystem as read-only

  # @schema description: Resource requests and limits
  resources:
    requests:
      cpu: 1 # @schema description: CPU request (e.g., "200m", "1", "2"); anyOf: [{"type": "string"}, {"type": "number"}]
      memory: 1Gi # @schema description: Memory request
    limits:
      cpu: 2 # @schema description: CPU limit (e.g., "200m", "1", "2"); anyOf: [{"type": "string"}, {"type": "number"}]
      memory: 4Gi # @schema description: Memory limit

  # @schema description: Node selector for pod assignment
  nodeSelector: {}

  # @schema description: Tolerations for pod assignment
  tolerations: []

  # @schema description: Affinity rules for pod assignment
  affinity: {}

  # @schema description: Ingress configuration
  ingress:
    enabled: false # @schema description: Enable ingress
    annotations: {} # @schema description: Annotations for the ingress
    hosts: [] # @schema description: Ingress hosts
    tls: [] # @schema description: TLS configuration

  # @schema description: Pod annotations
  podAnnotations: {}

  # @schema description: Pod labels
  podLabels: {}
